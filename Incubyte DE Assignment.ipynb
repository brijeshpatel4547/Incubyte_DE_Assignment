{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c443831-9fbc-4682-86bf-af0cc2b81eba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fad22c6-05f9-4fd0-8c26-25a00def626d",
     "showTitle": true,
     "title": "Importing Data"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"PartitionData\").getOrCreate()\n",
    "\n",
    "def ETL():\n",
    "\n",
    "    # Import Data\n",
    "    customer_df = spark.read.option(\"delimiter\",\"|\").option(\"header\", \"true\").csv(\"dbfs:/FileStore/shared_uploads/brijeshpatel4547@gmail.com/customer_data-1.csv\").drop('H')\n",
    "\n",
    "    # Converting Strings to Dates for better Processing\n",
    "    customer_df = customer_df.withColumn(\"Open_Date\", to_date(col(\"Open_Date\"),\"yyyyMMdd\")) \\\n",
    "            .withColumn(\"Last_Consulted_Date\", to_date(col(\"Last_Consulted_Date\"),\"yyyyMMdd\")) \\\n",
    "            .withColumn(\"DOB\", to_date(col(\"DOB\"),\"MMddyyyy\"))\n",
    "\n",
    "    # Adding Additional Derivative Columns\n",
    "    # Add Age\n",
    "    customer_df = customer_df.withColumn(\"Age\",(datediff(current_date(), col(\"DOB\")) / 365.25).cast(\"int\"))\n",
    "\n",
    "    # days since last consulted >30 \n",
    "    customer_df = customer_df.withColumn(\"Days_Since_Last_Consulted_>_30\", when((datediff(current_date(),\"Last_Consulted_Date\").cast(\"int\")) >= 30, True) \\\n",
    "                                    .otherwise(False))\n",
    "\n",
    "    # Store the output as different files\n",
    "    out_path = \"dbfs:/FileStore/shared_uploads/brijeshpatel4547@gmail.com/results\"\n",
    "    customer_df.write.partitionBy(\"Country\").format(\"csv\").mode(\"overwrite\").save(out_path)\n",
    "\n",
    "ETL()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Incubyte DE Assignment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
